{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following is an import of PyTorch libraries.\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from data_loader import NEFG3x3Set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize Hyperparameters\n",
    "\"\"\"\n",
    "batch_size = 5\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create dataloaders to feed data into the neural network\n",
    "Default MNIST dataset is used and standard train/test split is performed\n",
    "\"\"\"\n",
    "dataset = NEFG3x3Set(\"info_dat_charge.csv\",\n",
    "                     \"data/3x12_16_damp00\", \"dat_charge\", transform=True, device=device)\n",
    "length = len(dataset)\n",
    "train_split = math.floor(length*.7)\n",
    "test_split = length - train_split\n",
    "\n",
    "train_inds, test_inds = torch.utils.data.random_split(\n",
    "    dataset, [train_split, test_split], generator=torch.Generator().manual_seed(42))\n",
    "train_data = torch.utils.data.DataLoader(dataset=train_inds, batch_size=5,\n",
    "                                         shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(dataset=test_inds, batch_size=5,\n",
    "                                        shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "Initialize the network and the Adam optimizer\n",
    "\"\"\"\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data, \u001b[39m0\u001b[39m):\n\u001b[1;32m      9\u001b[0m         (inp,_ , imgs, _,_,_, vd, vg) \u001b[39m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m         imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:672\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    671\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    674\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataset.py:297\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/Projects/ML_NEGF/data_loader.py:39\u001b[0m, in \u001b[0;36mNEFG3x3Set.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m cmp_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx, \u001b[39m1\u001b[39m])\n\u001b[1;32m     37\u001b[0m tar_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[idx, \u001b[39m2\u001b[39m])\n\u001b[0;32m---> 39\u001b[0m inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtens(np\u001b[39m.\u001b[39;49mloadtxt(inp_name, dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m cmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtens(np\u001b[39m.\u001b[39mloadtxt(cmp_name, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     41\u001b[0m tar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtens(np\u001b[39m.\u001b[39mloadtxt(tar_name, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/numpy/lib/npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1336\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1339\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1340\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1341\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1343\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    996\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[1;32m   1000\u001b[0m         data, delimiter\u001b[39m=\u001b[39mdelimiter, comment\u001b[39m=\u001b[39mcomment, quote\u001b[39m=\u001b[39mquote,\n\u001b[1;32m   1001\u001b[0m         imaginary_unit\u001b[39m=\u001b[39mimaginary_unit,\n\u001b[1;32m   1002\u001b[0m         usecols\u001b[39m=\u001b[39musecols, skiplines\u001b[39m=\u001b[39mskiplines, max_rows\u001b[39m=\u001b[39mmax_rows,\n\u001b[1;32m   1003\u001b[0m         converters\u001b[39m=\u001b[39mconverters, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1004\u001b[0m         encoding\u001b[39m=\u001b[39mencoding, filelike\u001b[39m=\u001b[39mfilelike,\n\u001b[1;32m   1005\u001b[0m         byte_converters\u001b[39m=\u001b[39mbyte_converters)\n\u001b[1;32m   1007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for idx, data in enumerate(train_data, 0):\n",
    "        (inp,_ , imgs, _,_,_, vd, vg) = data\n",
    "        imgs = imgs.float().to(device)\n",
    "        vd_fin = torch.empty(imgs.shape).to(device)\n",
    "        vg_fin = torch.empty(imgs.shape).to(device)\n",
    "\n",
    "        for j in range(len(vd)):\n",
    "            vd_fin[j,:,:,:] = vd[j]\n",
    "            vg_fin[j,:,:,:] = vg[j]\n",
    "\n",
    "        imgs1 = torch.cat((imgs,vd_fin,vg_fin), dim=1)\n",
    "    #     # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out = net(imgs1)\n",
    "        \n",
    "\n",
    "    #     # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "    #     # kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "    #     loss = F.mse_loss(out, imgs, reduction='sum') #+ kl_divergence\n",
    "        \n",
    "    #     # Backpropagation based on the loss\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    # loss = 0\n",
    "    \n",
    "    # net.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for data in test_data:\n",
    "    #         (inp,_ , imgs, _,_,_, vd, vg) = data\n",
    "    #         imgs = imgs.float().to(device)\n",
    "    #         vd_fin = torch.empty(imgs.shape).to(device)\n",
    "    #         vg_fin = torch.empty(imgs.shape).to(device)\n",
    "\n",
    "    #         for j in range(len(vd)):\n",
    "    #             vd_fin[j,:,:,:] = vd[j]\n",
    "    #             vg_fin[j,:,:,:] = vg[j]\n",
    "\n",
    "    #         imgs1 = torch.cat((imgs,vd_fin,vg_fin), dim=1)\n",
    "                    \n",
    "    #         out = net(imgs1)\n",
    "    #         # kl_divergence = 0.5 * torch.sum(-1 - logVAR + mu.pow(2) + logVAR.exp())\n",
    "    #         loss += F.mse_loss(out, imgs, reduction='sum')# + kl_divergence\n",
    "        \n",
    "    #     loss =loss/len(test_data)\n",
    "    #     print('Epoch {}: Loss {}'.format(epoch,loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_data:\n",
    "        (inp, cmp, imgs, _,_,_, vd, vg) = data\n",
    "        imgs = imgs.float().to(device)\n",
    "        vd_fin = torch.empty(imgs.shape).to(device)\n",
    "        vg_fin = torch.empty(imgs.shape).to(device)\n",
    "\n",
    "        for j in range(len(vd)):\n",
    "            vd_fin[j,:,:,:] = vd[j]\n",
    "            vg_fin[j,:,:,:] = vg[j]\n",
    "\n",
    "        imgs1 = torch.cat((imgs,vd_fin,vg_fin), dim=1)\n",
    "        \n",
    "        cmp = cmp.float().to(device)\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        a = plt.imshow(np.squeeze(imgs[0].cpu()))\n",
    "        c_bar = plt.colorbar(a)\n",
    "        out, mu, logVAR = net(imgs1)\n",
    "        outimg = out[0].cpu()\n",
    "        plt.subplot(122)\n",
    "        a = plt.imshow(np.squeeze(outimg))\n",
    "        c_bar = plt.colorbar(a)\n",
    "        print(F.mse_loss(imgs[0], out[0], reduction='sum'))\n",
    "        print(F.mse_loss(imgs[0], cmp[0], reduction='sum'))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data in dataset:\n",
    "    (inp, cmp, imgs, _,_,_, vd, vg) = data\n",
    "    imgs = imgs.float().to(\"cpu\")\n",
    "    print(imgs[0])\n",
    "    a = plt.imshow(imgs[0].T)\n",
    "    plt.colorbar(a)\n",
    "    plt.show()\n",
    "    break\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
