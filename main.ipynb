{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "from data_loader import NEFG3x3Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Determine if any GPUs are available\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "dataset = NEFG3x3Set(\"info_dat_std_NEGFXY.csv\",\n",
    "                     \"data/3x12_16_damp00\", \"dat_std\")\n",
    "\n",
    "length = len(dataset)\n",
    "train_split = math.floor(length*.7)\n",
    "test_split = length - train_split\n",
    "batch_size = 50\n",
    "train_inds, test_inds = torch.utils.data.random_split(\n",
    "    dataset, [train_split, test_split], generator=torch.Generator().manual_seed(42))\n",
    "train_data = torch.utils.data.DataLoader(dataset=train_inds, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(dataset=test_inds, batch_size=batch_size,\n",
    "                                        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Convolutional Variational Autoencoder\n",
    "\"\"\"\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, imgChannels=3, layers = [16,32,64,128]):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        inChannels = imgChannels\n",
    "        outChannels = layers[0]\n",
    "        for i in layers:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.Conv2d(in_channels=inChannels, out_channels=i, kernel_size=5),\n",
    "                nn.BatchNorm2d(i),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            inChannels = i\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.mu = nn.Conv2d(inChannels,inChannels, 1)\n",
    "        self.logVar = nn.Conv2d(inChannels,inChannels, 1)\n",
    "        self.short = nn.Conv2d(imgChannels, 1, 1)\n",
    "\n",
    "        modules = []\n",
    "        layers.reverse()\n",
    "        for i in range(len(layers)-1):\n",
    "            inChannels = layers[i]\n",
    "            outChannels = layers[i+1]\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=inChannels, out_channels=outChannels, kernel_size=5),\n",
    "                nn.BatchNorm2d(outChannels),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=layers[-1], out_channels=1, kernel_size=5),\n",
    "                )\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        logVar = self.logVar(x)\n",
    "        return mu,logVar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparamaterize(self, mu, logVar):\n",
    "        std = torch.exp(0.5 * logVar)\n",
    "        eps = Variable(torch.randn_like(std))\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mu,logVar = self.encode(x)\n",
    "        # z = self.reparamaterize(mu,logVar)\n",
    "        # out = torch.add(self.decode(self.encoder(x)),self.short(x))\n",
    "        # out = self.decode(self.encoder(x))\n",
    "        out = torch.add(self.decode(self.encoder(x)),x[:,0,...].unsqueeze(1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 1.9720952052336473\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.9713, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1: Train Loss 0.5401190026448324\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.1521, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2: Train Loss 0.3690835111416303\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.1389, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3: Train Loss 0.28766748481071913\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.1388, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4: Train Loss 0.21290818601846695\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.1289, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 5: Train Loss 0.1618474400960482\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.1239, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 6: Train Loss 0.12438314580000363\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.1171, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 7: Train Loss 0.09871996996494439\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.1102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 8: Train Loss 0.07746421746336497\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0957, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 9: Train Loss 0.06019149577388397\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0841, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 10: Train Loss 0.04852489811869768\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.1007, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 11: Train Loss 0.03950969530985905\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.1113, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 12: Train Loss 0.03344454107662806\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0768, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 13: Train Loss 0.028661761576166518\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0704, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 14: Train Loss 0.02507993707863184\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0969, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 15: Train Loss 0.02218227367848158\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0738, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 16: Train Loss 0.019663675580746852\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0679, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 17: Train Loss 0.0178883565016664\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0595, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 18: Train Loss 0.016613784317786876\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0653, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 19: Train Loss 0.015814872566037454\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0515, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 20: Train Loss 0.014908428590458173\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0730, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 21: Train Loss 0.015673674965420596\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0737, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 22: Train Loss 0.014758436260028528\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0650, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 23: Train Loss 0.013026331623013202\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0449, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 24: Train Loss 0.012418192417289201\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0938, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 25: Train Loss 0.02323864808735939\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.1069, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 26: Train Loss 0.014132701308251573\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0609, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 27: Train Loss 0.012710167524906306\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0701, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 28: Train Loss 0.012767186782394465\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0496, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 29: Train Loss 0.011316423107368441\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0547, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 30: Train Loss 0.011520157299505977\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0551, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 31: Train Loss 0.010562428989662575\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0362, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 32: Train Loss 0.010719390389008017\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0734, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 33: Train Loss 0.010901374402097784\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0395, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 34: Train Loss 0.010413071534668025\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0361, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 35: Train Loss 0.010325165250553535\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0352, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 36: Train Loss 0.009698429324019413\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0484, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 37: Train Loss 0.010545099458585564\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0647, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 38: Train Loss 0.010705055883870674\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.1190, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 39: Train Loss 0.010232902203614894\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0393, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 40: Train Loss 0.009905460792092176\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0343, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 41: Train Loss 0.01120940071100799\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0583, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 42: Train Loss 0.010373541422618123\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0346, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 43: Train Loss 0.009268129477277398\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0362, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 44: Train Loss 0.009232948820751447\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0488, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 45: Train Loss 0.010276232219229523\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0656, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 46: Train Loss 0.009549104178754183\n",
      "CMP: tensor(0.0195, device='mps:0')\n",
      "out: tensor(0.0322, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 47: Train Loss 0.008522395993797826\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0474, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 48: Train Loss 0.008593077592265148\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0483, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 49: Train Loss 0.010230534584619679\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0610, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 50: Train Loss 0.00888893293001904\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0366, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 51: Train Loss 0.007967146740366634\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0265, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 52: Train Loss 0.008917984964612585\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0352, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 53: Train Loss 0.008540489549676959\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0245, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 54: Train Loss 0.007779086510149332\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0289, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 55: Train Loss 0.008187849826824207\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0326, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 56: Train Loss 0.008413696231750341\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0350, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 57: Train Loss 0.007744723787674537\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0539, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 58: Train Loss 0.007916877279058099\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0331, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 59: Train Loss 0.008155206137766631\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0270, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 60: Train Loss 0.007578387874393509\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0335, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 61: Train Loss 0.0076615365060906\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0246, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 62: Train Loss 0.007119598682038486\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0218, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 63: Train Loss 0.00749729099110342\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0316, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 64: Train Loss 0.007097697225757516\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0235, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 65: Train Loss 0.00882074784917327\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0384, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 66: Train Loss 0.00729284009251457\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0254, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 67: Train Loss 0.007024723499153669\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0416, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 68: Train Loss 0.006821603165008128\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0499, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 69: Train Loss 0.007624581277083892\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0225, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 70: Train Loss 0.006952653065896952\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0221, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 71: Train Loss 0.006675925956537517\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0237, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 72: Train Loss 0.006944336283665437\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0203, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 73: Train Loss 0.006235363535009897\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0345, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 74: Train Loss 0.006211856532340439\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0236, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 75: Train Loss 0.00686671721856468\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0599, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 76: Train Loss 0.006713146191591827\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0565, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 77: Train Loss 0.007830825616390659\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0262, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 78: Train Loss 0.006551497714379086\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0235, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 79: Train Loss 0.006705224639377915\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0282, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 80: Train Loss 0.005455104652075813\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0279, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 81: Train Loss 0.006998845608904958\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0289, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 82: Train Loss 0.005881574873525936\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0222, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 83: Train Loss 0.00763987278780685\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0196, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 84: Train Loss 0.006821297610608431\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0247, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 85: Train Loss 0.0061542517953337384\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0404, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 86: Train Loss 0.005554207618563221\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0174, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 87: Train Loss 0.0067993625729846265\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0235, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 88: Train Loss 0.006310054611486311\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0169, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 89: Train Loss 0.006116896348360639\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0185, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 90: Train Loss 0.005526532087689982\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0510, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 91: Train Loss 0.00609182883412219\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0178, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 92: Train Loss 0.005313706996205907\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0187, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 93: Train Loss 0.0049030714972804375\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0172, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 94: Train Loss 0.005280712879119584\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0522, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 95: Train Loss 0.005611026507372467\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0383, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 96: Train Loss 0.005451730980824392\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0246, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 97: Train Loss 0.0043832485904344\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0151, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 98: Train Loss 0.0048140388232870744\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0318, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 99: Train Loss 0.004279561170663398\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0158, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 100: Train Loss 0.004680421642171076\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0328, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 101: Train Loss 0.004899783552481005\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0141, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 102: Train Loss 0.0052784912897130614\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0336, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 103: Train Loss 0.004072376652262532\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0145, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 104: Train Loss 0.004252740298397839\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0148, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 105: Train Loss 0.0042501258878753735\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0439, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 106: Train Loss 0.004101711334302449\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0145, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 107: Train Loss 0.0035941914583627996\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0175, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 108: Train Loss 0.003937905945349485\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0120, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 109: Train Loss 0.0036857812575852643\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0147, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 110: Train Loss 0.0036852316966710184\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0111, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 111: Train Loss 0.0043224077313565295\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0205, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 112: Train Loss 0.0036269651296047065\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0321, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 113: Train Loss 0.003996052340461085\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0141, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 114: Train Loss 0.004887159533189753\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0690, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 115: Train Loss 0.0040368898234401755\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0114, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 116: Train Loss 0.0038165706388938883\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0105, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 117: Train Loss 0.004425384498272951\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0182, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 118: Train Loss 0.0040248919260473205\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0366, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 119: Train Loss 0.003899394883774221\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0139, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 120: Train Loss 0.0035472222281476627\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0137, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 121: Train Loss 0.0036118382211917867\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0128, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 122: Train Loss 0.003154023765371396\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0080, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 123: Train Loss 0.003326848189597233\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0108, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 124: Train Loss 0.0033759185910009993\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0181, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 125: Train Loss 0.0037280322912220773\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 126: Train Loss 0.003566078890938885\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0662, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 127: Train Loss 0.006451549653250437\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0138, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 128: Train Loss 0.004961500042834534\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0149, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 129: Train Loss 0.0036594048500634157\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0100, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 130: Train Loss 0.00313094594121839\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0089, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 131: Train Loss 0.0030817571722974\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0105, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 132: Train Loss 0.004288663290655957\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0111, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 133: Train Loss 0.004090099613397167\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0265, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 134: Train Loss 0.010062713005866569\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0233, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 135: Train Loss 0.0038838705334525844\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0120, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 136: Train Loss 0.003952955446528414\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 137: Train Loss 0.0031489020911976695\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0119, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 138: Train Loss 0.0033510366159204682\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0109, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 139: Train Loss 0.0029626126902607772\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0103, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 140: Train Loss 0.0026178627964467383\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0070, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 141: Train Loss 0.0033482209546491504\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0321, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 142: Train Loss 0.003389680905876538\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0106, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 143: Train Loss 0.0029379248063868056\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0087, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 144: Train Loss 0.0027947291606464065\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0208, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 145: Train Loss 0.0027577129479211112\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0079, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 146: Train Loss 0.003209480805358348\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0220, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 147: Train Loss 0.003495926623984885\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0159, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 148: Train Loss 0.003568106890620234\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0152, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 149: Train Loss 0.0027689254090476493\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0112, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 150: Train Loss 0.004251555176989104\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0424, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 151: Train Loss 0.0040206303820014\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0147, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 152: Train Loss 0.0027647876726964917\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0097, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 153: Train Loss 0.003105207619508012\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0198, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 154: Train Loss 0.0026363808473643777\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0111, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 155: Train Loss 0.0042667387707087286\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0320, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 156: Train Loss 0.0031367074116133153\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 157: Train Loss 0.0028057143521996643\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0126, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 158: Train Loss 0.003576687778919362\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0164, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 159: Train Loss 0.004468418939862973\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0232, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 160: Train Loss 0.0026641238095740285\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0142, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 161: Train Loss 0.0031459950304661807\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0119, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 162: Train Loss 0.0031681507781076315\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0495, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 163: Train Loss 0.0024636636013523317\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0091, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 164: Train Loss 0.0022304213041654574\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 165: Train Loss 0.009054119537512843\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0685, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 166: Train Loss 0.007325144574189415\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0370, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 167: Train Loss 0.004065031678272555\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0206, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 168: Train Loss 0.0041646533108388\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0764, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 169: Train Loss 0.0036904287094680164\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0093, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 170: Train Loss 0.0027257360947819855\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0147, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 171: Train Loss 0.003519539886978097\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0118, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 172: Train Loss 0.004100339670773023\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0704, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 173: Train Loss 0.004266181723393787\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0254, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 174: Train Loss 0.0029834302930304636\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0084, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 175: Train Loss 0.002961302386560979\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 176: Train Loss 0.0030532379527218067\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0094, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 177: Train Loss 0.0033667084062471986\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0451, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 178: Train Loss 0.0040684795195165165\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0799, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 179: Train Loss 0.004421836723430226\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0327, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 180: Train Loss 0.0031849305243947757\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0108, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 181: Train Loss 0.0023260864903792166\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0078, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 182: Train Loss 0.0024500828003510833\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0065, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 183: Train Loss 0.0022266060382557604\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0063, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 184: Train Loss 0.0026552225910843564\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0115, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 185: Train Loss 0.0028009822932430184\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0107, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 186: Train Loss 0.0026011413518482675\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0114, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 187: Train Loss 0.0022935391350004533\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0124, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 188: Train Loss 0.002344746550079435\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0076, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 189: Train Loss 0.002079105756890315\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0066, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 190: Train Loss 0.0022384239844261454\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0092, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 191: Train Loss 0.002008768564197593\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0048, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 192: Train Loss 0.002184294835807612\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0071, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 193: Train Loss 0.002331216139002488\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0103, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 194: Train Loss 0.002003537410368713\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0060, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 195: Train Loss 0.002044894634029613\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0072, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 196: Train Loss 0.0019160859116639656\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0057, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 197: Train Loss 0.002232826587994798\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0101, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 198: Train Loss 0.0021498289962227526\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0074, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 199: Train Loss 0.0018626638741877216\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0054, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 200: Train Loss 0.002207129078809745\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0098, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 201: Train Loss 0.0036072057958405753\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0118, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 202: Train Loss 0.004037530555461462\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0313, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 203: Train Loss 0.0030316760575470445\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0671, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 204: Train Loss 0.0032240704794485983\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0172, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 205: Train Loss 0.002914439573382529\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0085, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 206: Train Loss 0.0021940550247493843\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0058, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 207: Train Loss 0.0019122299541217776\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0061, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 208: Train Loss 0.00192868186143012\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0070, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 209: Train Loss 0.0018001316751066882\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0061, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 210: Train Loss 0.002025395063146089\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0054, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 211: Train Loss 0.001767219527839468\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0061, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 212: Train Loss 0.001825552021798033\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0051, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 213: Train Loss 0.0018166875278648848\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0105, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 214: Train Loss 0.0018143742501986427\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0043, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 215: Train Loss 0.0016127887602823859\n",
      "CMP: tensor(0.0194, device='mps:0')\n",
      "out: tensor(0.0041, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 216: Train Loss 0.0028867733512575235\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0288, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 217: Train Loss 0.002658367931592063\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0080, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 218: Train Loss 0.002041418521772497\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0116, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 219: Train Loss 0.0025482264553340008\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0158, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 220: Train Loss 0.002036782962162621\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0088, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 221: Train Loss 0.0020378318060046206\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0089, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 222: Train Loss 0.0018091986690146418\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0050, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 223: Train Loss 0.001807676019290319\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 224: Train Loss 0.0017203517759648652\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0064, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 225: Train Loss 0.001759926515380637\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0046, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 226: Train Loss 0.0021377898161657727\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0090, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 227: Train Loss 0.002472343944156399\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0097, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 228: Train Loss 0.0019402533474091727\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 229: Train Loss 0.0016384400301971114\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0059, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 230: Train Loss 0.0017744863311795948\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0068, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 231: Train Loss 0.0018755670574207145\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0067, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 232: Train Loss 0.0019237515292703533\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0082, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 233: Train Loss 0.0019735917269897004\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0072, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 234: Train Loss 0.0016918939386064617\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 235: Train Loss 0.0016670855559193743\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 236: Train Loss 0.0016552282165950881\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0054, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 237: Train Loss 0.00193313140502701\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 238: Train Loss 0.0016346988402521955\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0056, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 239: Train Loss 0.0015505611484583754\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 240: Train Loss 0.0030537956850961424\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0376, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 241: Train Loss 0.003826716189415982\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0311, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 242: Train Loss 0.003413784013201411\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0113, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 243: Train Loss 0.002133670961484313\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0058, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 244: Train Loss 0.0027794062580841663\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0525, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 245: Train Loss 0.003313458332111343\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0091, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 246: Train Loss 0.002377890699650519\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0373, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 247: Train Loss 0.002921325374000634\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0135, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 248: Train Loss 0.0021225013635837687\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0094, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 249: Train Loss 0.0019179659519487848\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0056, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 250: Train Loss 0.0019860922587283244\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0051, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 251: Train Loss 0.0016610020941768128\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0038, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 252: Train Loss 0.001658720947139395\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 253: Train Loss 0.0016202602553396271\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 254: Train Loss 0.001833586830448789\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0045, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 255: Train Loss 0.002046754030743614\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0111, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 256: Train Loss 0.003056282441740712\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0262, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 257: Train Loss 0.0030630701176750544\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0097, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 258: Train Loss 0.0018472468229727102\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0044, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 259: Train Loss 0.0017431294164942722\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0121, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 260: Train Loss 0.0020212909976880136\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0105, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 261: Train Loss 0.0020407190470275683\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0054, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 262: Train Loss 0.0019191131219626046\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 263: Train Loss 0.0018849537725775288\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0059, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 264: Train Loss 0.0023602858552924143\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0121, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 265: Train Loss 0.0027733989185295426\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0068, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 266: Train Loss 0.0016701809385827242\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 267: Train Loss 0.002142824093775394\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0063, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 268: Train Loss 0.0017909869448675846\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 269: Train Loss 0.0015120801869600725\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 270: Train Loss 0.0015052693197503686\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 271: Train Loss 0.0020094017068353984\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0115, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 272: Train Loss 0.0020212492717501637\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0457, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 273: Train Loss 0.0019078304758295417\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0085, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 274: Train Loss 0.0015553122972890448\n",
      "CMP: tensor(0.0194, device='mps:0')\n",
      "out: tensor(0.0064, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 275: Train Loss 0.0016258978956522276\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 276: Train Loss 0.0019593634642660618\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0053, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 277: Train Loss 0.003207301823959614\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0580, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 278: Train Loss 0.0069159191782371355\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0328, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 279: Train Loss 0.0036919516009780075\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0105, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 280: Train Loss 0.0038696520988686155\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0487, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 281: Train Loss 0.0022506973249479556\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0074, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 282: Train Loss 0.0018480824476752717\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0051, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 283: Train Loss 0.0016008989254120165\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 284: Train Loss 0.0017052460797668363\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0046, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 285: Train Loss 0.0017345843617267047\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 286: Train Loss 0.0014825332708334406\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 287: Train Loss 0.0015045588795776265\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 288: Train Loss 0.0014662961878527242\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 289: Train Loss 0.0014306924690922292\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 290: Train Loss 0.0013766607533817967\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 291: Train Loss 0.001829526801324951\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0093, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 292: Train Loss 0.0031829900153052923\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0104, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 293: Train Loss 0.003841211338742421\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0128, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 294: Train Loss 0.0028694913713619686\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0100, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 295: Train Loss 0.0016756992352804025\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 296: Train Loss 0.002278186790555572\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0107, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 297: Train Loss 0.002402966327141397\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0048, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 298: Train Loss 0.0024311962725523\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0229, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 299: Train Loss 0.002842147964446877\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0136, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 300: Train Loss 0.002202313975431025\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 301: Train Loss 0.001720478836572371\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0046, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 302: Train Loss 0.0014292874100367324\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0045, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 303: Train Loss 0.0014442191101037539\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 304: Train Loss 0.0013787525217944325\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0033, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 305: Train Loss 0.0013181835977145685\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 306: Train Loss 0.0012787816432627062\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0033, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 307: Train Loss 0.0014056849714296942\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0048, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 308: Train Loss 0.001373020535818516\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0038, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 309: Train Loss 0.001776665057360123\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0103, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 310: Train Loss 0.0015564830167792165\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 311: Train Loss 0.0013411326933867084\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 312: Train Loss 0.0012515138883967525\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 313: Train Loss 0.0013072567033724715\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0033, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 314: Train Loss 0.0016542870927458773\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0038, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 315: Train Loss 0.0011974510114389257\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 316: Train Loss 0.001257161430727977\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 317: Train Loss 0.0012175234307785733\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 318: Train Loss 0.0016722810060645526\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 319: Train Loss 0.0016779373198425253\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0278, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 320: Train Loss 0.0020319737108925786\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0069, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 321: Train Loss 0.002988768401197516\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0175, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 322: Train Loss 0.0017682352694324576\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0059, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 323: Train Loss 0.0014495811729620283\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0046, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 324: Train Loss 0.0013938104608454383\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 325: Train Loss 0.0016141889142230726\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0033, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 326: Train Loss 0.0015967229712539567\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0063, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 327: Train Loss 0.0013793636574589002\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 328: Train Loss 0.0016339484651465542\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 329: Train Loss 0.0013539905932982666\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 330: Train Loss 0.0013723486985187405\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0044, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 331: Train Loss 0.0012129941897001117\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 332: Train Loss 0.0013409816870430054\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 333: Train Loss 0.0012609191068734687\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 334: Train Loss 0.0011115121350695307\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 335: Train Loss 0.0012222051938386778\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 336: Train Loss 0.0011452569093447751\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 337: Train Loss 0.0011734001835485776\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0041, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 338: Train Loss 0.0013930336034928376\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0138, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 339: Train Loss 0.0016464012934683035\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 340: Train Loss 0.0014006984375345593\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 341: Train Loss 0.0011986272987157393\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 342: Train Loss 0.0012270180114473288\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 343: Train Loss 0.0012586221721274061\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0043, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 344: Train Loss 0.0010560430147541831\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 345: Train Loss 0.001210363769832139\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 346: Train Loss 0.001133566809585318\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 347: Train Loss 0.0011345132773455519\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 348: Train Loss 0.0012460294439314078\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0044, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 349: Train Loss 0.001712095173738467\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0070, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 350: Train Loss 0.0024567035725340247\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0092, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 351: Train Loss 0.0012372202651861769\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 352: Train Loss 0.0011364593855642641\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 353: Train Loss 0.0011913882626686245\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 354: Train Loss 0.0019492921150790958\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0048, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 355: Train Loss 0.0017704634989898365\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0057, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 356: Train Loss 0.001374065661086486\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 357: Train Loss 0.0011796651994630408\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 358: Train Loss 0.0013853061760668284\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 359: Train Loss 0.0011501331553937723\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 360: Train Loss 0.0010587490631643538\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 361: Train Loss 0.0010718524460501682\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0047, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 362: Train Loss 0.0013045904454954255\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 363: Train Loss 0.0011127559118904173\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 364: Train Loss 0.0014350644711297578\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 365: Train Loss 0.0011240050505595999\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 366: Train Loss 0.0011571770435414063\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 367: Train Loss 0.0018146367219742388\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0084, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 368: Train Loss 0.0016109370201145513\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0048, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 369: Train Loss 0.0025993087284195307\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0075, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 370: Train Loss 0.0015416655693955433\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0062, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 371: Train Loss 0.001259222528180824\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 372: Train Loss 0.0012296938131420086\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 373: Train Loss 0.0012275928949328284\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 374: Train Loss 0.0012537807432146599\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0042, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 375: Train Loss 0.0009702586595757076\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 376: Train Loss 0.0014217175126899607\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0045, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 377: Train Loss 0.0011204700702084945\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 378: Train Loss 0.0010174483222027237\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 379: Train Loss 0.0009972709253466187\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 380: Train Loss 0.00388768180542124\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0211, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 381: Train Loss 0.0032063885681474437\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0099, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 382: Train Loss 0.0016287856224852686\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 383: Train Loss 0.0013380018018114453\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 384: Train Loss 0.0012941166149595608\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 385: Train Loss 0.0011821537224862438\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 386: Train Loss 0.001114594671749868\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0051, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 387: Train Loss 0.0010896816361659707\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 388: Train Loss 0.0010611320635339676\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 389: Train Loss 0.0017025096189732163\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 390: Train Loss 0.0011070239841221617\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 391: Train Loss 0.0010084325667076672\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 392: Train Loss 0.001023461054921007\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 393: Train Loss 0.0009795370950506856\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 394: Train Loss 0.0009241545429596534\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 395: Train Loss 0.0010003763667415255\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 396: Train Loss 0.001453957809224868\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0037, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 397: Train Loss 0.00272952756495215\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0128, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 398: Train Loss 0.0034853023467943645\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0133, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 399: Train Loss 0.0031561133756230655\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0133, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 400: Train Loss 0.0019237837404944003\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 401: Train Loss 0.001389147924223485\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 402: Train Loss 0.0012918254830695402\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0067, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 403: Train Loss 0.0011943845960419052\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 404: Train Loss 0.0010421079193922477\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 405: Train Loss 0.0011709773008568357\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 406: Train Loss 0.0012365466435977186\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 407: Train Loss 0.0019073832283118884\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0285, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 408: Train Loss 0.0032943793473980175\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0155, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 409: Train Loss 0.0035289400346720447\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0067, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 410: Train Loss 0.001586639732032871\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 411: Train Loss 0.0013117818049907398\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 412: Train Loss 0.0017314071278983296\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0075, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 413: Train Loss 0.0014708018548285158\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 414: Train Loss 0.0012900114704210025\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 415: Train Loss 0.0011253646539094357\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 416: Train Loss 0.0011152254860812367\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 417: Train Loss 0.0014818666811781721\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 418: Train Loss 0.001034585279949869\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 419: Train Loss 0.0009556732861360965\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 420: Train Loss 0.001006191195651459\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 421: Train Loss 0.001083063951227814\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0351, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 422: Train Loss 0.001438627796372972\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 423: Train Loss 0.0011063497334432143\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 424: Train Loss 0.0009971311105451046\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0020, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 425: Train Loss 0.0011518946468221168\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 426: Train Loss 0.0009954335893361042\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 427: Train Loss 0.0009666239292038461\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0024, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 428: Train Loss 0.0010725726270510887\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 429: Train Loss 0.0009641578429951691\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 430: Train Loss 0.0010166336589743597\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 431: Train Loss 0.0009347354778303549\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 432: Train Loss 0.0019455136677536827\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 433: Train Loss 0.002734963746311573\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0054, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 434: Train Loss 0.0013259136332915379\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0038, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 435: Train Loss 0.001054791543328275\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 436: Train Loss 0.0009484884257499988\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0020, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 437: Train Loss 0.0008890241554651696\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 438: Train Loss 0.0009530893786667058\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 439: Train Loss 0.0008880777481513527\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 440: Train Loss 0.0008743660067781233\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 441: Train Loss 0.000884360502823256\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 442: Train Loss 0.000967867851543885\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0022, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 443: Train Loss 0.0009637864825960535\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 444: Train Loss 0.0008613830723334104\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 445: Train Loss 0.0008611925060484702\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 446: Train Loss 0.001068502438451665\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 447: Train Loss 0.0009209432240683013\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 448: Train Loss 0.0009022509251595833\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0022, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 449: Train Loss 0.0008645985340430903\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 450: Train Loss 0.0012071415549144149\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 451: Train Loss 0.0017545090880818092\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0192, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 452: Train Loss 0.0012593880640521932\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 453: Train Loss 0.0009231351387615388\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0019, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 454: Train Loss 0.0008410847110029024\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0017, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 455: Train Loss 0.0009146980659212344\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0025, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 456: Train Loss 0.0009224652626121846\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 457: Train Loss 0.0009105596848082944\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 458: Train Loss 0.0008350606546558153\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0019, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 459: Train Loss 0.0008733756840229034\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0020, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 460: Train Loss 0.0008575885708873662\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 461: Train Loss 0.0009353076647340248\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 462: Train Loss 0.0008868914222917878\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 463: Train Loss 0.0009079153152505079\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 464: Train Loss 0.0008823176568302398\n",
      "CMP: tensor(0.0186, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 465: Train Loss 0.0008700223787365338\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 466: Train Loss 0.0008658745635050134\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 467: Train Loss 0.0008873444480391649\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 468: Train Loss 0.0011825779220089316\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 469: Train Loss 0.0032031276809553113\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0098, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 470: Train Loss 0.0018388930117138303\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0045, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 471: Train Loss 0.0011604093821146167\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0032, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 472: Train Loss 0.0012470691111117888\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0034, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 473: Train Loss 0.001794946104592572\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0079, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 474: Train Loss 0.0011974087750646644\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 475: Train Loss 0.0009071174023959499\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 476: Train Loss 0.0009137489407574042\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0023, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 477: Train Loss 0.0010351911444628898\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 478: Train Loss 0.0010670565870196486\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 479: Train Loss 0.0014607395412615286\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0101, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 480: Train Loss 0.0010752503361660414\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 481: Train Loss 0.0016662296436082285\n",
      "CMP: tensor(0.0192, device='mps:0')\n",
      "out: tensor(0.0204, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 482: Train Loss 0.0015910183675049876\n",
      "CMP: tensor(0.0193, device='mps:0')\n",
      "out: tensor(0.0061, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 483: Train Loss 0.0012008654482017916\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 484: Train Loss 0.001094067968481865\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0036, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 485: Train Loss 0.002633379091723607\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0645, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 486: Train Loss 0.0121171881731313\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0682, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 487: Train Loss 0.004969353966701489\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0132, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 488: Train Loss 0.004772569482716231\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0210, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 489: Train Loss 0.0021883733481025468\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0061, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 490: Train Loss 0.0015872541698627174\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0055, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 491: Train Loss 0.001479821848289038\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 492: Train Loss 0.0014564641370760421\n",
      "CMP: tensor(0.0194, device='mps:0')\n",
      "out: tensor(0.0029, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 493: Train Loss 0.0011522299766791267\n",
      "CMP: tensor(0.0189, device='mps:0')\n",
      "out: tensor(0.0026, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 494: Train Loss 0.0023642060964798126\n",
      "CMP: tensor(0.0185, device='mps:0')\n",
      "out: tensor(0.0053, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 495: Train Loss 0.0013313200257611102\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0040, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 496: Train Loss 0.0011076256767130243\n",
      "CMP: tensor(0.0188, device='mps:0')\n",
      "out: tensor(0.0027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 497: Train Loss 0.0010185523200421953\n",
      "CMP: tensor(0.0187, device='mps:0')\n",
      "out: tensor(0.0022, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 498: Train Loss 0.0009109464926251138\n",
      "CMP: tensor(0.0191, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 499: Train Loss 0.0009716678513751293\n",
      "CMP: tensor(0.0190, device='mps:0')\n",
      "out: tensor(0.0021, device='mps:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = VAE(imgChannels=6).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "num_epochs = 500\n",
    "loss = 0\n",
    "kl_divergence1 = 0\n",
    "min_loss = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    loss1 = 0\n",
    "    kl_divergence1 = 0\n",
    "    net.train()\n",
    "    for idx, data in enumerate(train_data, 0):\n",
    "        out = net(data[0])\n",
    "        \n",
    "        # kl_divergence = batch_size *0.5* torch.mean(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.mse_loss(out, data[3].unsqueeze(1))# + kl_divergence\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss1 += loss.item()\n",
    "        # kl_divergence1+=kl_divergence\n",
    "    loss1 = loss1/len(test_data)\n",
    "    # kl_divergence1=kl_divergence1/len(test_data)\n",
    "    # print(kl_divergence1)\n",
    "    print('Epoch {}: Train Loss {}'.format(epoch,loss1))\n",
    "    l1=0\n",
    "    l2=0\n",
    "    net.eval()\n",
    "    for data in test_data:\n",
    "        out = net(data[0])    \n",
    "        cmp = data[1]\n",
    "        # print((data[3].shape))\n",
    "        l1+=F.mse_loss(data[3], cmp)\n",
    "        l2+=F.mse_loss(data[3].unsqueeze(1), out)\n",
    "    if l2<min_loss:\n",
    "        min_loss = l2\n",
    "        torch.save(net.state_dict(),\"trained_run1.sd\")\n",
    "    print(\"CMP:\", l1)\n",
    "    print(\"out:\", l2)\n",
    "\n",
    "# Epoch 15: Train Loss 0.36227205101 kl-div 0-1\n",
    "# \n",
    "# Epoch 15: Train Loss 0.99161257338 kl-div std\n",
    "\n",
    "# Epoch 15: Train Loss 0.30827124285 kl-div std\n",
    "\n",
    "# Use 0-1 data - maybe\n",
    "# Use whole range for layer 6 - X\n",
    "# Remove charge just use pot\n",
    "\n",
    "# Epoch 7: Train Loss 0.0947228386425055\n",
    "# Epoch 7: Train Loss 0.0694243241674625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMP: tensor(0.0004, device='mps:0')\n",
      "out: tensor(0.0001, device='mps:0')\n",
      "inp: 0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "l2=0\n",
    "l1=0\n",
    "l3=0\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_data:\n",
    "        out = net(data[0])    \n",
    "        cmp = data[1]\n",
    "        # print((data[3].shape))\n",
    "        l1+=F.mse_loss(data[3], cmp)\n",
    "        l2+=F.mse_loss(data[3].unsqueeze(1), out)\n",
    "        plt.imshow(out[3].squeeze().cpu())\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(data[3][3].cpu())\n",
    "        plt.show()\n",
    "        break\n",
    "        # l3+=F.l1_loss(data[3], data[0][0].unsqueeze(1))\n",
    "\n",
    "print(\"CMP:\", l1)\n",
    "print(\"out:\", l2)\n",
    "print(\"inp:\", l3)\n",
    "\n",
    "# 0.0021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
