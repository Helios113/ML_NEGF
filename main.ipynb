{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "from data_loader import NEFGSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Determine if any GPUs are available\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "dataset = NEFG3x3Set(\"info_dat_std_NEGFXY.csv\",\n",
    "                     \"data/3x12_16_damp00\", \"dat_std\")\n",
    "\n",
    "length = len(dataset)\n",
    "train_split = math.floor(length*.7)\n",
    "test_split = length - train_split\n",
    "batch_size = 50\n",
    "train_inds, test_inds = torch.utils.data.random_split(\n",
    "    dataset, [train_split, test_split], generator=torch.Generator().manual_seed(42))\n",
    "train_data = torch.utils.data.DataLoader(dataset=train_inds, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(dataset=test_inds, batch_size=batch_size,\n",
    "                                        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Convolutional Variational Autoencoder\n",
    "\"\"\"\n",
    "class VAE(nn.Module):\n",
    "    # ther og net had 128 final channels\n",
    "    def __init__(self, imgChannels=3, layers = [16,32,64,64]):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        inChannels = imgChannels\n",
    "        outChannels = layers[0]\n",
    "        for i in layers:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.Conv2d(in_channels=inChannels, out_channels=i, kernel_size=5),\n",
    "                nn.BatchNorm2d(i),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            inChannels = i\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.mu = nn.Conv2d(inChannels,inChannels, 1)\n",
    "        self.logVar = nn.Conv2d(inChannels,inChannels, 1)\n",
    "        self.short = nn.Conv2d(imgChannels, 1, 1)\n",
    "\n",
    "        modules = []\n",
    "        layers.reverse()\n",
    "        for i in range(len(layers)-1):\n",
    "            inChannels = layers[i]\n",
    "            outChannels = layers[i+1]\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=inChannels, out_channels=outChannels, kernel_size=5),\n",
    "                nn.BatchNorm2d(outChannels),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        modules.append(\n",
    "                nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=layers[-1], out_channels=1, kernel_size=5),\n",
    "                )\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        logVar = self.logVar(x)\n",
    "        return mu,logVar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparamaterize(self, mu, logVar):\n",
    "        std = torch.exp(0.5 * logVar)\n",
    "        eps = Variable(torch.randn_like(std))\n",
    "        return eps * std + mu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # mu,logVar = self.encode(x)\n",
    "        # z = self.reparamaterize(mu,logVar)\n",
    "        # out = torch.add(self.decode(z),x[:,0,...].unsqueeze(1))\n",
    "        # out = torch.add(self.decode(self.encoder(x)),self.short(x))\n",
    "        # out = self.decode(self.encoder(x))\n",
    "        out = torch.add(self.decode(self.encoder(x)),x[:,0,...].unsqueeze(1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 1.5795950454014998\n",
      "out: tensor(0.5254, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1: Train Loss 0.5868437358966241\n",
      "CMP: tensor(0.0036, device='mps:0')\n",
      "out: tensor(0.1638, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2: Train Loss 0.43109608728152055\n",
      "out: tensor(0.1558, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3: Train Loss 0.32820409822922486\n",
      "out: tensor(0.1335, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4: Train Loss 0.2548791215969966\n",
      "out: tensor(0.1284, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 5: Train Loss 0.19417580732932457\n",
      "out: tensor(0.1103, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 6: Train Loss 0.14939492339125046\n",
      "out: tensor(0.1098, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 7: Train Loss 0.11498325432722385\n",
      "out: tensor(0.0881, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 8: Train Loss 0.09122978036220257\n",
      "out: tensor(0.1068, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 9: Train Loss 0.07349599396380094\n",
      "out: tensor(0.1027, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 10: Train Loss 0.05624877102673054\n",
      "out: tensor(0.0787, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 11: Train Loss 0.044981583093221374\n",
      "out: tensor(0.0976, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 12: Train Loss 0.03943098981219988\n",
      "out: tensor(0.0776, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 13: Train Loss 0.030817619453255948\n",
      "out: tensor(0.0694, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 14: Train Loss 0.024714962149468753\n",
      "out: tensor(0.0817, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 15: Train Loss 0.02283312240615487\n",
      "out: tensor(0.1114, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 16: Train Loss 0.01957485119167429\n",
      "out: tensor(0.0590, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 17: Train Loss 0.016918522126686115\n",
      "out: tensor(0.0533, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 18: Train Loss 0.015474594054886928\n",
      "out: tensor(0.0534, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 19: Train Loss 0.01441270843721353\n",
      "out: tensor(0.0593, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 20: Train Loss 0.013643614679145126\n",
      "out: tensor(0.0599, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 21: Train Loss 0.013957050557319935\n",
      "out: tensor(0.0462, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 22: Train Loss 0.01219190127001359\n",
      "out: tensor(0.0496, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 23: Train Loss 0.01118455845146225\n",
      "out: tensor(0.0448, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 24: Train Loss 0.011920897809502024\n",
      "out: tensor(0.0443, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 25: Train Loss 0.011451520025730133\n",
      "out: tensor(0.0418, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 26: Train Loss 0.0128246508180522\n",
      "out: tensor(0.0395, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 27: Train Loss 0.009702638149834596\n",
      "out: tensor(0.0374, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 28: Train Loss 0.010651452001184225\n",
      "out: tensor(0.0408, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 29: Train Loss 0.009408148184705239\n",
      "out: tensor(0.0794, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 30: Train Loss 0.011170145768958788\n",
      "out: tensor(0.0469, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 31: Train Loss 0.009788581152231647\n",
      "out: tensor(0.0336, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 32: Train Loss 0.009252854049778901\n",
      "out: tensor(0.0422, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 33: Train Loss 0.0098177429754287\n",
      "out: tensor(0.0507, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 34: Train Loss 0.01006310979405848\n",
      "out: tensor(0.0447, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 35: Train Loss 0.01008675111314425\n",
      "out: tensor(0.0384, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 36: Train Loss 0.009218018806467835\n",
      "out: tensor(0.0351, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 37: Train Loss 0.008620815691896357\n",
      "out: tensor(0.0294, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 38: Train Loss 0.00857364864518436\n",
      "out: tensor(0.0491, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 39: Train Loss 0.01081948161411744\n",
      "out: tensor(0.0797, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 40: Train Loss 0.009041153951189838\n",
      "out: tensor(0.0298, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 41: Train Loss 0.00858882855838881\n",
      "out: tensor(0.0306, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 42: Train Loss 0.007789524057163642\n",
      "out: tensor(0.0271, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 43: Train Loss 0.00789242497502038\n",
      "out: tensor(0.0533, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 44: Train Loss 0.008636307544433154\n",
      "out: tensor(0.0375, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 45: Train Loss 0.00844879151107027\n",
      "out: tensor(0.0280, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 46: Train Loss 0.008422902462860713\n",
      "out: tensor(0.0312, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 47: Train Loss 0.0073370715030110795\n",
      "out: tensor(0.0230, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 48: Train Loss 0.0069946335861459374\n",
      "out: tensor(0.0241, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 49: Train Loss 0.007320207078009844\n",
      "out: tensor(0.0239, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 50: Train Loss 0.0066971511741240435\n",
      "out: tensor(0.0241, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 51: Train Loss 0.0076551381140374224\n",
      "out: tensor(0.0353, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 52: Train Loss 0.00810553738847375\n",
      "out: tensor(0.0515, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 53: Train Loss 0.006931422475295572\n",
      "out: tensor(0.0258, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 54: Train Loss 0.006625014867705221\n",
      "out: tensor(0.0231, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 55: Train Loss 0.005672281043818937\n",
      "out: tensor(0.0213, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 56: Train Loss 0.006704847256724651\n",
      "out: tensor(0.0541, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 57: Train Loss 0.008277884684503078\n",
      "out: tensor(0.0282, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 58: Train Loss 0.007230977894953237\n",
      "out: tensor(0.0384, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 59: Train Loss 0.007710600943447879\n",
      "out: tensor(0.0213, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 60: Train Loss 0.006225892638142865\n",
      "out: tensor(0.0293, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 61: Train Loss 0.006023525135018504\n",
      "out: tensor(0.0186, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 62: Train Loss 0.005404543740531573\n",
      "out: tensor(0.0202, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 63: Train Loss 0.005869687660239064\n",
      "out: tensor(0.0248, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 64: Train Loss 0.005420226845532083\n",
      "out: tensor(0.0188, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 65: Train Loss 0.006085504885189808\n",
      "out: tensor(0.0297, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 66: Train Loss 0.0062949820421636105\n",
      "out: tensor(0.0205, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 67: Train Loss 0.006404411307392785\n",
      "out: tensor(0.0196, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 68: Train Loss 0.005865190187110924\n",
      "out: tensor(0.0214, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 69: Train Loss 0.006146250991150737\n",
      "out: tensor(0.0225, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 70: Train Loss 0.005400980527226169\n",
      "out: tensor(0.0237, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 71: Train Loss 0.00461189781064884\n",
      "out: tensor(0.0168, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 72: Train Loss 0.00433176785456733\n",
      "out: tensor(0.0152, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 73: Train Loss 0.004968786576332955\n",
      "out: tensor(0.0181, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 74: Train Loss 0.004556513570535641\n",
      "out: tensor(0.0189, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 75: Train Loss 0.004975500489728382\n",
      "out: tensor(0.0189, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 76: Train Loss 0.004418873254997799\n",
      "out: tensor(0.0136, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 77: Train Loss 0.004230979668836181\n",
      "out: tensor(0.0157, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 78: Train Loss 0.004411724413960026\n",
      "out: tensor(0.0163, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 79: Train Loss 0.004551130391728993\n",
      "out: tensor(0.0140, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 80: Train Loss 0.004164245866167431\n",
      "out: tensor(0.0143, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 81: Train Loss 0.004760166818204408\n",
      "out: tensor(0.0212, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 82: Train Loss 0.0065981632170195766\n",
      "out: tensor(0.0339, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 83: Train Loss 0.00537412182893604\n",
      "out: tensor(0.0381, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 84: Train Loss 0.004963983656265414\n",
      "out: tensor(0.0194, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 85: Train Loss 0.004054468760911662\n",
      "out: tensor(0.0135, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 86: Train Loss 0.004994253296619997\n",
      "out: tensor(0.0341, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 87: Train Loss 0.005748454463453247\n",
      "out: tensor(0.0154, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 88: Train Loss 0.004634016739706008\n",
      "out: tensor(0.0261, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 89: Train Loss 0.0040605145712526375\n",
      "out: tensor(0.0113, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 90: Train Loss 0.004184609601417413\n",
      "out: tensor(0.0143, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 91: Train Loss 0.007532806726745688\n",
      "out: tensor(0.0293, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 92: Train Loss 0.00795035134965124\n",
      "out: tensor(0.0276, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 93: Train Loss 0.005787962984938461\n",
      "out: tensor(0.0196, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 94: Train Loss 0.004256136384076224\n",
      "out: tensor(0.0162, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 95: Train Loss 0.004688291671650054\n",
      "out: tensor(0.0162, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 96: Train Loss 0.004035105158646519\n",
      "out: tensor(0.0143, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 97: Train Loss 0.0038508440618618177\n",
      "out: tensor(0.0126, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 98: Train Loss 0.0033219689682412604\n",
      "out: tensor(0.0114, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 99: Train Loss 0.0034398571856749747\n",
      "out: tensor(0.0116, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 100: Train Loss 0.00410321714858023\n",
      "out: tensor(0.0141, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 101: Train Loss 0.0042006571669704635\n",
      "out: tensor(0.0139, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 102: Train Loss 0.0038101725289239907\n",
      "out: tensor(0.0129, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 103: Train Loss 0.0034478909174840036\n",
      "out: tensor(0.0136, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 104: Train Loss 0.003310963430871757\n",
      "out: tensor(0.0113, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 105: Train Loss 0.0031580372417990407\n",
      "out: tensor(0.0112, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 106: Train Loss 0.0029409524465266327\n",
      "out: tensor(0.0106, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 107: Train Loss 0.0040010506591687985\n",
      "out: tensor(0.0107, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 108: Train Loss 0.003895521911684997\n",
      "out: tensor(0.0134, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 109: Train Loss 0.004299605211529594\n",
      "out: tensor(0.0112, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 110: Train Loss 0.003243087764041355\n",
      "out: tensor(0.0108, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 111: Train Loss 0.003013647515147638\n",
      "out: tensor(0.0097, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 112: Train Loss 0.0028117345466923257\n",
      "out: tensor(0.0075, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 113: Train Loss 0.0031133842958997074\n",
      "out: tensor(0.0111, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 114: Train Loss 0.003735239433948524\n",
      "out: tensor(0.0255, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 115: Train Loss 0.006336055104978955\n",
      "out: tensor(0.0585, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 116: Train Loss 0.004337741845609764\n",
      "out: tensor(0.0238, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 117: Train Loss 0.0036809542843212304\n",
      "out: tensor(0.0140, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 118: Train Loss 0.00349812206471912\n",
      "out: tensor(0.0089, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 119: Train Loss 0.0037159811965046595\n",
      "out: tensor(0.0141, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 120: Train Loss 0.0030720537403025306\n",
      "out: tensor(0.0077, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 121: Train Loss 0.002731759347415601\n",
      "out: tensor(0.0087, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 122: Train Loss 0.0029389465120262825\n",
      "out: tensor(0.0096, device='mps:0', grad_fn=<AddBackward0>)\n",
      "Epoch 123: Train Loss 0.0027849236861444437\n",
      "out: tensor(0.0101, device='mps:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m kl_divergence1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data, \u001b[39m0\u001b[39m):\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m net(data[\u001b[39m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m     \u001b[39m# kl_divergence = batch_size *0.5* torch.mean(-1 - logVar + mu.pow(2) + logVar.exp())\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:672\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    671\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    674\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/utils/data/dataset.py:297\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/Projects/ML_NEGF/data_loader.py:46\u001b[0m, in \u001b[0;36mNEFG3x3Set.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# self.labels.iloc[idx, 2] =  self.labels.iloc[idx, 2][:-5]+\"3.txt\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# self.labels.iloc[idx, 3] =  self.labels.iloc[idx, 2][:-5]+\"3.txt\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39m# print(self.labels.iloc[idx, 2])\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     dat\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49mloadtxt(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_dir,\u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels\u001b[39m.\u001b[39;49miloc[idx, i\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m])), dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m dat\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/numpy/lib/npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1336\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1339\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1340\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1341\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1343\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/torch-gpu/lib/python3.11/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    996\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[1;32m   1000\u001b[0m         data, delimiter\u001b[39m=\u001b[39mdelimiter, comment\u001b[39m=\u001b[39mcomment, quote\u001b[39m=\u001b[39mquote,\n\u001b[1;32m   1001\u001b[0m         imaginary_unit\u001b[39m=\u001b[39mimaginary_unit,\n\u001b[1;32m   1002\u001b[0m         usecols\u001b[39m=\u001b[39musecols, skiplines\u001b[39m=\u001b[39mskiplines, max_rows\u001b[39m=\u001b[39mmax_rows,\n\u001b[1;32m   1003\u001b[0m         converters\u001b[39m=\u001b[39mconverters, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1004\u001b[0m         encoding\u001b[39m=\u001b[39mencoding, filelike\u001b[39m=\u001b[39mfilelike,\n\u001b[1;32m   1005\u001b[0m         byte_converters\u001b[39m=\u001b[39mbyte_converters)\n\u001b[1;32m   1007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "net = VAE(imgChannels=7).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "num_epochs = 500\n",
    "loss = 0\n",
    "kl_divergence1 = 0\n",
    "min_loss = 1000\n",
    "l1=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss1 = 0\n",
    "    kl_divergence1 = 0\n",
    "    net.train()\n",
    "    for idx, data in enumerate(train_data, 0):\n",
    "        out = net(data[0])\n",
    "        \n",
    "        # kl_divergence = batch_size *0.5* torch.mean(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.mse_loss(out, data[3].unsqueeze(1))# + kl_divergence\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss1 += loss.item()\n",
    "        # kl_divergence1+=kl_divergence\n",
    "    loss1 = loss1/len(test_data)\n",
    "    # kl_divergence1=kl_divergence1/len(test_data)\n",
    "    # print(kl_divergence1)\n",
    "    print('Epoch {}: Train Loss {}'.format(epoch,loss1))\n",
    "    l2=0\n",
    "    net.eval()\n",
    "    for data in test_data:\n",
    "        out = net(data[0])    \n",
    "        if epoch == 1:\n",
    "            cmp = data[1]\n",
    "            # print((data[3].shape))\n",
    "            l1+=F.mse_loss(data[3], cmp)\n",
    "        l2+=F.mse_loss(data[3].unsqueeze(1), out)\n",
    "    if l2<min_loss:\n",
    "        min_loss = l2\n",
    "        torch.save(net.state_dict(),\"trained_run2.sd\")\n",
    "    if epoch == 1:\n",
    "        print(\"CMP:\", l1)\n",
    "    print(\"out:\", l2)\n",
    "\n",
    "# Epoch 15: Train Loss 0.36227205101 kl-div 0-1\n",
    "# \n",
    "# Epoch 15: Train Loss 0.99161257338 kl-div std\n",
    "\n",
    "# Epoch 15: Train Loss 0.30827124285 kl-div std\n",
    "\n",
    "# Use 0-1 data - yes\n",
    "# Use whole range for layer 6 - X\n",
    "# Remove charge just use pot\n",
    "\n",
    "# Epoch 7: Train Loss 0.0947228386425055\n",
    "# Epoch 7: Train Loss 0.0694243241674625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "l2=0\n",
    "l1=0\n",
    "l3=0\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_data:\n",
    "        out = net(data[0])           \n",
    "        cmp = data[0][23][5]\n",
    "        plt.imshow(cmp.cpu())#, cmap=\"inferno\")\n",
    "        y = np.linspace(0,cmp.shape[0],7)[1:-1]\n",
    "        x = np.linspace(0,cmp.shape[1],5)[1:-1]\n",
    "        plt.hlines(y,0,cmp.shape[1]-1, colors='white')\n",
    "        plt.vlines(x,0,cmp.shape[0]-1 ,colors='white')\n",
    "        for x in np.linspace(0,cmp.shape[1]-1, 9)[1::2]:\n",
    "            for i, y in enumerate(np.linspace(0,cmp.shape[0]-1, 13)[1::2]):\n",
    "                plt.annotate(str(i/5),[x,y],xytext=(-7, -3), textcoords='offset points', color='white', weight=\"bold\")\n",
    "        plt.axis('off')\n",
    "        print(np.linspace(0,cmp.shape[0]-1, 13)[1::2])\n",
    "        plt.savefig(\"grad.svg\",  format=\"svg\",bbox_inches=\"tight\", pad_inches=0)\n",
    "        break\n",
    "        # # print((data[3].shape))\n",
    "        # l1+=F.mse_loss(data[3], cmp)\n",
    "        # l2+=F.mse_loss(data[3].unsqueeze(1), out)\n",
    "        # plt.imshow(out[3].squeeze().cpu())\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.imshow(data[3][3].cpu())\n",
    "        # plt.show()\n",
    "        # break\n",
    "        # l3+=F.l1_loss(data[3], data[0][0].unsqueeze(1))\n",
    "\n",
    "print(\"CMP:\", l1)\n",
    "print(\"out:\", l2)\n",
    "print(\"inp:\", l3)\n",
    "\n",
    "# 0.0021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
